{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -qq transformers\n",
    "# ! pip install -qq wandb\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "                          AutoModelWithLMHead,\n",
    "                          Trainer,\n",
    "                          AutoTokenizer,\n",
    "                          TextDataset,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          TrainingArguments,)\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.tsv', sep='\\t').fillna('')\n",
    "val_df = pd.read_csv('dev.tsv', sep='\\t').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforming dataframe to pairs toxic-neutral and adding special tokens for paraphrasing\n",
    "    \"\"\"\n",
    "    df_toxic = []\n",
    "    df_neutral = []\n",
    "    for index, row in df.iterrows():\n",
    "        references = row[['neutral_comment1', 'neutral_comment2',\n",
    "                          'neutral_comment3']].tolist()\n",
    "        for reference in references:\n",
    "            if len(reference) > 0:\n",
    "                df_toxic.append(row['toxic_comment'])\n",
    "                df_neutral.append(reference)\n",
    "            else:\n",
    "                break\n",
    "    df = pd.DataFrame({'toxic_comment': df_toxic, 'neutral_comment': df_neutral})\n",
    "    df['input'] = '<s>' + df.neutral_comment + '</s>>>>><p>' + df.toxic_comment + '</p>'\n",
    "    return df\n",
    "\n",
    "df = prepare_df(df).sample(frac = 1)\n",
    "val_df = prepare_df(val_df)\n",
    "df['input'].to_csv('combined.txt', sep='\\n', index=False)\n",
    "val_df['input'].to_csv('combined_dev.txt', sep='\\n', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg= {\n",
    "    'text_path' :'/content/toxic_only_train.txt',\n",
    "    'text_path_dev' : '/content/toxic_only_val.txt',\n",
    "    'output_dir' : '/models/gpt/',\n",
    "    'tokenizer_output_path': '/tokenizers/gpt',\n",
    "    'block_size' : 128,\n",
    "    'epochs' : 6,\n",
    "    'batch_size' : 10,\n",
    "    'warmup_steps' : 600,\n",
    "    'save_steps' : 600,\n",
    "    'logging_steps' : 100,\n",
    "    'max_length' : 100,\n",
    "    'model_path' : \"/models/gpt/\",#'/content/output/checkpoint-1800/',\n",
    "    'tokenizer_path': 'tokenizers/gpt/',\n",
    "    'weight_decay' : 1e-6,\n",
    "    'learning_rate' : 3e-5,\n",
    "    'lr_scheduler_type' : 'cosine_with_restarts',\n",
    "    }\n",
    "\n",
    "run = wandb.init(project=\"toxify\", config=cfg, entity=\"username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = cfg['output_dir'],\n",
    "    num_train_epochs = cfg['epochs'],\n",
    "    per_device_train_batch_size = cfg['batch_size'],\n",
    "    warmup_steps=cfg['warmup_steps'],\n",
    "    save_steps=cfg['save_steps'],\n",
    "    logging_steps=cfg['logging_steps'],\n",
    "    weight_decay = cfg['weight_decay'],\n",
    "    lr_scheduler_type = cfg['lr_scheduler_type'],\n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=cfg['learning_rate'],\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(cfg['model_path'], max_length = cfg['max_length'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg['tokenizer_path'])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "train_dataset = TextDataset(tokenizer=tokenizer,file_path=cfg['text_path'], block_size=cfg['block_size'])\n",
    "dev_dataset = TextDataset(tokenizer=tokenizer,file_path=cfg['text_path_dev'], block_size=cfg['block_size']) \n",
    "\n",
    "trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=dev_dataset,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(cfg['output_dir'])\n",
    "model.save_pretrained(cfg['tokenizer_output_path'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
