{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -qq transformers\n",
    "# ! pip install -qq wandb\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "                          AutoModelWithLMHead,\n",
    "                          Trainer,\n",
    "                          AutoTokenizer,\n",
    "                          TextDataset,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          TrainingArguments,)\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic1 = pd.read_csv('labeled.csv')\n",
    "toxic1 = toxic1[toxic1.toxic > 0 ]\n",
    "len(toxic1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/alexandersemiletov/starter-read-toxic-russian-comments-dataset\n",
    "data_list = []\n",
    "with open(\"dataset.txt\") as file:\n",
    "    for line in file:\n",
    "        labels = line.split()[0]\n",
    "        text = line[len(labels)+1:].strip()\n",
    "        labels = labels.split(\",\")\n",
    "        mask = [1 if \"__label__NORMAL\" in labels else 0,\n",
    "                1 if \"__label__INSULT\" in labels else 0,\n",
    "                1 if \"__label__THREAT\" in labels else 0,\n",
    "                1 if \"__label__OBSCENITY\" in labels else 0]\n",
    "        data_list.append((text, *mask))\n",
    "toxic2 = pd.DataFrame(data_list, columns=[\"text\", \"normal\", \"insult\", \"threat\", \"obscenity\"])\n",
    "toxic2['toxic'] = toxic2[['insult','threat','obscenity']].sum(axis=1)\n",
    "print(len(toxic2))\n",
    "toxic2 = toxic2[toxic2.toxic > 0]\n",
    "print(len(toxic2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "toxic = pd.concat([toxic2['text'],toxic1['comment']])\n",
    "toxic_train, toxic_val = train_test_split(toxic, test_size = 0.1)\n",
    "toxic_train.to_csv('toxic_only_train.txt', sep='\\n', index=False)\n",
    "toxic_val.to_csv('toxic_only_val.txt', sep='\\n', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg= {\n",
    "    'text_path' :'/content/toxic_only_train.txt',\n",
    "    'text_path_dev' : '/content/toxic_only_val.txt',\n",
    "    'output_path': './models/gpt/',\n",
    "    'tokenizer_output_path': './tokenizers/gpt',\n",
    "    'block_size' : 128,\n",
    "    'epochs' : 2,\n",
    "    'batch_size' : 20,\n",
    "    'warmup_steps' : 400,\n",
    "    'save_steps' : 700,\n",
    "    'logging_steps' : 100,\n",
    "    'max_length' : 100,\n",
    "    'model_name' : \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
    "    'weight_decay' : 1e-6,\n",
    "    'learning_rate' : 4e-5,\n",
    "    'lr_scheduler_type' : 'cosine_with_restarts',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = cfg['output_path'],\n",
    "    num_train_epochs = cfg['epochs'],\n",
    "    per_device_train_batch_size = cfg['batch_size'],\n",
    "    warmup_steps=cfg['warmup_steps'],\n",
    "    save_steps=cfg['save_steps'],\n",
    "    logging_steps=cfg['logging_steps'],\n",
    "    weight_decay = cfg['weight_decay'],\n",
    "    lr_scheduler_type = cfg['lr_scheduler_type'],\n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=cfg['learning_rate'],\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "  \n",
    "run = wandb.init(project=\"detox_russe_gpt\", config=cfg, entity=\"mikezz1\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(cfg['model_name'], max_length = cfg['max_length'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg['model_name'])\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "train_dataset = TextDataset(tokenizer=tokenizer,file_path=cfg['text_path'], block_size=cfg['block_size'])\n",
    "dev_dataset = TextDataset(tokenizer=tokenizer,file_path=cfg['text_path_dev'], block_size=cfg['block_size']) \n",
    "\n",
    "trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=dev_dataset,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(cfg['output_path'])\n",
    "model.save_pretrained(cfg['tokenizer_output_path'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
